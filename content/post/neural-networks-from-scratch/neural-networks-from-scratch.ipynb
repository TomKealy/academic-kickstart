{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful sometimes to write simple versions of complex things, so that you understand them. In this post we write a simple neural network from scratch.\n",
    "\n",
    "In a normal classification problem, we have some labels \\(y\\) and inputs \\(x\\) and we would like to learn a linear function \n",
    "\n",
    "\n",
    "$$ y = W x $$ \n",
    "\n",
    "to separate the classes. Neural networks add an (or many!) extra layer \n",
    "\n",
    "\n",
    "$$ h = \\mathrm{sigmoid}(M x) $$\n",
    "\n",
    "between the inputs and output so that it produces is\n",
    "\n",
    "$$ y = W h $$\n",
    "\n",
    "Thus we are esentially fitting a linear classifier on the basis expansion \\(\\mathrm{sigmoid}(M x)\\), the difference being that w efit the basis expansion, as well as the linear classifier. That is the Network learns a data dependent basis on which to clssify.\n",
    "\n",
    "Enough with the maths, lets do some coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are made up of Layers, the simplest just returns what it recieves as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"This is an identity layer so it doesn't need to do anything.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        output: Tensor of shape [batch_size, num_output_units]\n",
    "\n",
    "        \"\"\"\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        grad_output : Tensor of shape  [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        grad_output : Tensor of shape [batch_size, num_output_units]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add some non-linearity layers: a ReLU layer, and a Sigmoid layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        output: Tensor of shape [batch_size, num_output_units]\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(0, input)\n",
    "        \n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. ReLU input\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        grad_output : Tensor of shape  [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        grad_output : Tensor of shape [batch_size, num_output_units]\n",
    "        \"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"Sigmoid layer simply applies elementwise sigmoid unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        output: Tensor of shape [batch_size, num_output_units]\n",
    "\n",
    "        \"\"\"\n",
    "        return np.tanh(input)\n",
    "        \n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Compute gradient of loss w.r.t. ReLU input\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        grad_output : Tensor of shape  [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        grad_output : Tensor of shape [batch_size, num_output_units]\n",
    "        \"\"\"\n",
    "        sigmoid_grad = 1 - np.tanh(input)*np.tanh(input)\n",
    "        return grad_output*sigmoid_grad        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this by evaluating the numerical gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/ (our ysda course).\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Sigmoid()\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next type of layer we will implement will be a Dense or Fully Connected layer. Unlike nonlinearity, this layer actually has something to learn.\n",
    "\n",
    "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
    "$$f(X)= W \\cdot X + \\vec b $$\n",
    "\n",
    "Where \n",
    "* X is an object-feature matrix of shape [batch_size, num_features],\n",
    "* W is a weight matrix [num_features, num_outputs] \n",
    "* and b is a vector of num_outputs biases.\n",
    "\n",
    "Both W and b are initialized during layer creation and updated each time backward is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \n",
    "        Weights initialised by Xavier initialisation: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.randn(input_units, output_units) * np.sqrt(2.0/(input_units+output_units))\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        output: Tensor of shape [batch_size, num_output_units]\n",
    "        \"\"\"\n",
    "        return input @ self.weights + self.biases\n",
    "    \n",
    "    def backward(self, input, grad_output):  \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor of shape [batch_size, num_input_units]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        grad_output: Tensor of shape [batch_size, num_output_units]\n",
    "        \"\"\"\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "        \n",
    "        grad_weights = (input.T @ grad_output)\n",
    "        grad_biases = grad_output.sum(axis=0)\n",
    "  \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "    \n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, some tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Dense(128, 150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Dense(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Dense(32,64,learning_rate=0)\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out_given_wb(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return l.forward(x)\n",
    "    \n",
    "def compute_grad_by_params(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    l.backward(x,np.ones([10,64]) / 10.)\n",
    "    return w - l.weights, b - l.biases\n",
    "    \n",
    "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
    "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
    "grad_w,grad_b = compute_grad_by_params(w,b)\n",
    "\n",
    "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will optimise the following loss, which is a more numerically stable version of logg loss (courtesy of Coursera advanced ML):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50)%10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
    "\n",
    "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to load the mnist dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(flatten=False):\n",
    "    import keras\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # normalize x\n",
    "    X_train = X_train.astype(float) / 255.\n",
    "    X_test = X_test.astype(float) / 255.\n",
    "\n",
    "    # we reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        network = []\n",
    "        network.append(Dense(X_train.shape[1],100))\n",
    "        network.append(ReLU())\n",
    "        network.append(Dense(100,200))\n",
    "        network.append(ReLU())\n",
    "        network.append(Dense(200,10))\n",
    "        self.network = network\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        comppute activations of all network layers by applying them sequentially.\n",
    "        Return a list of activations for each layer. \n",
    "        Make sure last activation corresponds to network logits.\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        input = X\n",
    "\n",
    "        for layer in self.network:\n",
    "            activations.append(layer.forward(input))\n",
    "            input = activations[-1]\n",
    "        \n",
    "        assert len(activations) == len(self.network)\n",
    "        return activations\n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Compute network predictions.\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)[-1]\n",
    "        return logits.argmax(axis=-1)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        Train your network on a given batch of X and y.\n",
    "        You first need to run forward to get all layer activations.\n",
    "        Then you can run layer.backward going from last to first layer.\n",
    "    \n",
    "        After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Get the layer activations\n",
    "        layer_activations = self.forward(X)\n",
    "        layer_inputs = [X]+layer_activations\n",
    "        logits = layer_activations[-1]\n",
    "    \n",
    "        # Compute the loss and the initial gradient\n",
    "        loss = softmax_crossentropy_with_logits(logits,y)\n",
    "        loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "\n",
    "        for layer_i in range(len(self.network))[::-1]:\n",
    "            layer = self.network[layer_i]\n",
    "        \n",
    "            loss_grad = layer.backward(layer_inputs[layer_i],loss_grad) \n",
    "            \n",
    "        return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our nework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24\n",
      "Train accuracy: 1.0\n",
      "Val accuracy: 0.9819\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126e74240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "network = Network()\n",
    "train_log = []\n",
    "val_log = []\n",
    "\n",
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        network.fit(x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(network.predict(X_train)==y_train))\n",
    "    val_log.append(np.mean(network.predict(X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nikola": {
   "category": "",
   "date": "2018-03-25 11:07:02 UTC+02:00",
   "description": "",
   "link": "",
   "slug": "neural-networks-from-scratch",
   "tags": "",
   "title": "Neural Networks from Scratch",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
